# Resume-Robot 算法需求与优化方案

## 一、核心算法需求分析

### 1.1 项目核心流程中的算法

```
简历接入 → 简历解析（LLM）→ 预筛选（规则引擎）→ 岗位匹配（算法+LLM）→ 推荐报告生成
   ↓            ↓              ↓                ↓                    ↓
 去重算法    特征提取      规则匹配算法      匹配度融合算法        排序算法
```

## 二、关键算法需求

### 2.1 简历去重算法（Resume Deduplication）

**当前实现**：
- 使用MD5文件哈希进行去重
- 问题：相同内容不同格式的简历无法识别（如：PDF和Word版本）

**需要优化**：
1. **内容相似度检测**
   - 使用文本相似度算法（如：Jaccard相似度、编辑距离）
   - 提取简历关键信息（姓名、电话、邮箱、工作经历）进行相似度计算
   - 阈值：相似度 > 0.95 视为重复简历

2. **特征向量相似度**
   - 使用Embedding向量计算简历相似度
   - 提取简历核心特征（技能、经验、教育背景）生成向量
   - 使用Milvus进行向量相似度搜索

**算法选择**：
- **文本相似度**：Jaccard相似度、编辑距离（Levenshtein距离）
- **向量相似度**：余弦相似度（Cosine Similarity）
- **特征提取**：使用LLM提取简历关键特征，生成结构化特征向量

### 2.2 匹配度融合算法（Match Score Fusion）

**当前实现**：
- 简单的加权平均：`score = vector_weight * vector_score + rule_weight * rule_score + llm_weight * llm_score`
- 问题：权重固定，无法根据岗位特点动态调整

**需要优化**：
1. **动态权重调整**
   - 根据岗位类型调整权重（技术岗位更看重技能匹配，管理岗位更看重经验匹配）
   - 根据匹配模型配置调整权重
   - 根据历史匹配效果反馈调整权重

2. **多维度融合**
   - 向量相似度（技能、经验匹配）
   - 规则匹配度（硬门槛匹配）
   - LLM深度分析（综合匹配度）
   - 组织匹配度（部门、文化匹配）
   - 时间衰减因子（简历新鲜度）

3. **融合算法选择**
   - **加权平均**：简单但有效
   - **加权几何平均**：对低分更敏感
   - **加权调和平均**：对高分更敏感
   - **机器学习融合**：使用历史数据训练融合模型（可选，后续优化）

**算法公式**：
```python
# 基础融合
final_score = (
    w1 * vector_similarity +      # 向量相似度（0-1）
    w2 * rule_match_score +      # 规则匹配度（0-1）
    w3 * llm_analysis_score +     # LLM分析分数（0-10，归一化到0-1）
    w4 * org_match_score          # 组织匹配度（0-1，新增）
) * 10  # 转换为0-10分

# 时间衰减
if resume_age_days > 90:
    final_score *= (1 - (resume_age_days - 90) * 0.01)  # 超过90天的简历衰减
```

### 2.3 向量相似度计算（Vector Similarity）

**当前实现**：
- 使用Milvus进行向量存储和搜索
- 问题：向量生成方式不明确，向量维度可能不匹配

**需要优化**：
1. **向量生成策略**
   - **岗位向量**：岗位描述 + 岗位要求 + 部门信息 + 公司信息
   - **简历向量**：工作经历 + 技能 + 教育背景 + 项目经验
   - 使用统一的Embedding模型（如：DeepSeek Embedding、千问Embedding）

2. **向量维度统一**
   - 确保岗位向量和简历向量使用相同的Embedding模型
   - 向量维度：768维（或根据模型调整）

3. **相似度计算**
   - **余弦相似度**（Cosine Similarity）：适合文本向量
   - **欧氏距离**（Euclidean Distance）：可选
   - **点积相似度**（Dot Product）：如果向量已归一化

**算法实现**：
```python
def calculate_vector_similarity(resume_vector, job_vector):
    """
    计算向量相似度
    
    使用余弦相似度：
    similarity = (A · B) / (||A|| * ||B||)
    """
    import numpy as np
    
    # 余弦相似度
    dot_product = np.dot(resume_vector, job_vector)
    norm_resume = np.linalg.norm(resume_vector)
    norm_job = np.linalg.norm(job_vector)
    
    if norm_resume == 0 or norm_job == 0:
        return 0.0
    
    cosine_similarity = dot_product / (norm_resume * norm_job)
    
    # 归一化到0-1范围（余弦相似度范围是-1到1，但文本向量通常在0-1）
    return max(0.0, cosine_similarity)
```

### 2.4 批量处理优化算法（Batch Processing）

**当前实现**：
- 顺序处理，效率低
- 问题：批量上传100份简历，需要很长时间

**需要优化**：
1. **并发处理**
   - 使用异步任务队列（如：Celery + Redis）
   - 并发解析：同时处理多个简历
   - 并发匹配：同时匹配多个简历到岗位

2. **任务调度算法**
   - **优先级队列**：高优先级任务先处理
   - **负载均衡**：根据服务器负载分配任务
   - **任务分片**：大任务拆分成小任务并行处理

3. **批处理优化**
   - **批量向量化**：一次生成多个向量（如果API支持）
   - **批量LLM调用**：合并多个小请求（如果API支持）
   - **缓存优化**：相同内容的简历复用解析结果

**算法设计**：
```python
# 并发处理示例
async def batch_parse_resumes(resume_ids: List[int], max_concurrent: int = 5):
    """
    批量解析简历（并发处理）
    
    使用信号量控制并发数，避免API限流
    """
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def parse_single(resume_id):
        async with semaphore:
            return await parse_resume(resume_id)
    
    tasks = [parse_single(rid) for rid in resume_ids]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    return results
```

### 2.5 排序算法（Ranking Algorithm）

**当前实现**：
- 简单的按匹配度分数排序
- 问题：没有考虑其他因素（如：简历新鲜度、候选人活跃度）

**需要优化**：
1. **多因素排序**
   - **主要因素**：匹配度分数（降序）
   - **次要因素**：简历更新时间（降序，优先新简历）
   - **辅助因素**：候选人活跃度（如果有）

2. **排序算法**
   - **多键排序**：先按匹配度，再按时间
   - **加权排序**：综合多个因素计算排序分数

**算法实现**：
```python
def calculate_ranking_score(match_score: float, resume_age_days: int) -> float:
    """
    计算排序分数
    
    综合考虑匹配度和简历新鲜度
    """
    # 匹配度权重：0.9
    # 新鲜度权重：0.1（简历越新，分数越高）
    freshness_score = max(0, 1 - resume_age_days / 365)  # 一年内的简历新鲜度
    
    ranking_score = 0.9 * match_score + 0.1 * freshness_score
    
    return ranking_score
```

### 2.6 规则匹配算法（Rule Matching）

**当前实现**：
- 简单的规则检查（学历、经验、技能等）
- 问题：规则组合逻辑可能不够灵活

**需要优化**：
1. **规则引擎优化**
   - 支持复杂的规则组合（AND/OR/NOT）
   - 支持规则优先级
   - 支持规则依赖（规则A的结果影响规则B）

2. **规则匹配算法**
   - **决策树**：根据规则构建决策树
   - **规则链**：按优先级顺序执行规则
   - **规则缓存**：缓存规则匹配结果

### 2.7 文本特征提取算法（Feature Extraction）

**当前实现**：
- 使用LLM提取特征
- 问题：可能不够结构化，难以量化

**需要优化**：
1. **结构化特征提取**
   - 技能特征向量（技术栈、工具、框架）
   - 经验特征向量（行业、公司规模、项目类型）
   - 教育特征向量（学校、专业、学历）

2. **特征工程**
   - **TF-IDF**：提取关键词特征
   - **N-gram**：提取短语特征
   - **命名实体识别（NER）**：提取人名、地名、公司名等

### 2.8 异常检测算法（Anomaly Detection）

**需要新增**：
1. **简历质量检测**
   - 检测简历是否完整（关键信息缺失）
   - 检测简历是否异常（如：工作经历时间重叠、年龄不合理）

2. **匹配异常检测**
   - 检测匹配结果是否异常（如：分数异常高/低）
   - 检测匹配结果是否一致（多次匹配结果差异大）

**算法选择**：
- **统计方法**：Z-score、IQR（四分位距）
- **机器学习**：Isolation Forest、One-Class SVM（可选，后续优化）

## 三、算法优化优先级

### 3.1 高优先级（核心功能）

1. **匹配度融合算法优化** ⭐⭐⭐
   - 影响：匹配精度
   - 优化：动态权重、多维度融合

2. **向量相似度计算优化** ⭐⭐⭐
   - 影响：匹配精度
   - 优化：统一向量生成、优化相似度计算

3. **批量处理优化** ⭐⭐
   - 影响：处理效率
   - 优化：并发处理、任务调度

### 3.2 中优先级（体验优化）

4. **简历去重算法优化** ⭐⭐
   - 影响：数据质量
   - 优化：内容相似度检测、特征向量相似度

5. **排序算法优化** ⭐
   - 影响：用户体验
   - 优化：多因素排序

6. **规则匹配算法优化** ⭐
   - 影响：筛选精度
   - 优化：复杂规则组合、规则缓存

### 3.3 低优先级（后续优化）

7. **文本特征提取优化**
8. **异常检测算法**
9. **机器学习融合模型**（使用历史数据训练）

## 四、具体优化方案

### 4.1 匹配度融合算法优化

**当前问题**：
- 权重固定，无法动态调整
- 融合方式单一（加权平均）

**优化方案**：

```python
class MatchScoreFusion:
    """匹配度融合算法"""
    
    def __init__(self, match_model: Optional[MatchModel] = None):
        self.match_model = match_model
        self.default_weights = {
            "vector": 0.3,
            "rule": 0.2,
            "llm": 0.5
        }
    
    def calculate_final_score(
        self,
        vector_similarity: float,      # 0-1
        rule_match_result: Dict,       # 规则匹配结果
        llm_analysis: Dict,            # LLM分析结果
        org_match_score: float = 0.0,  # 组织匹配度（新增）
        job_type: str = None           # 岗位类型（用于动态调整权重）
    ) -> Tuple[float, Dict[str, Any]]:
        """
        计算最终匹配分数
        
        Returns:
            (最终分数, 评分明细)
        """
        # 1. 获取权重配置
        if self.match_model:
            weights = self.match_model.model_config
        else:
            weights = self.default_weights.copy()
        
        # 2. 动态调整权重（根据岗位类型）
        if job_type:
            weights = self._adjust_weights_by_job_type(weights, job_type)
        
        # 3. 计算各维度分数
        vector_score = vector_similarity * 10  # 转换为0-10分
        
        rule_score = self._calculate_rule_score(rule_match_result) * 10
        
        llm_score = llm_analysis.get("score", 0.0)  # 已经是0-10分
        
        org_score = org_match_score * 10  # 转换为0-10分
        
        # 4. 融合算法（加权平均 + 时间衰减）
        final_score = (
            weights.get("vector_weight", 0.3) * vector_score +
            weights.get("rule_weight", 0.2) * rule_score +
            weights.get("llm_weight", 0.5) * llm_score +
            weights.get("org_weight", 0.0) * org_score  # 新增组织匹配权重
        )
        
        # 5. 时间衰减（可选）
        # final_score = self._apply_time_decay(final_score, resume_age_days)
        
        # 6. 生成匹配标签
        match_label = self._generate_match_label(final_score)
        
        return final_score, {
            "vector_score": vector_score,
            "rule_score": rule_score,
            "llm_score": llm_score,
            "org_score": org_score,
            "final_score": final_score,
            "match_label": match_label,
            "weights": weights
        }
    
    def _adjust_weights_by_job_type(self, weights: Dict, job_type: str) -> Dict:
        """根据岗位类型动态调整权重"""
        # 技术岗位：更看重技能匹配（向量相似度）
        if "技术" in job_type or "研发" in job_type or "开发" in job_type:
            weights["vector_weight"] = 0.4
            weights["llm_weight"] = 0.4
            weights["rule_weight"] = 0.2
        
        # 管理岗位：更看重经验匹配（LLM分析）
        elif "管理" in job_type or "总监" in job_type or "经理" in job_type:
            weights["vector_weight"] = 0.2
            weights["llm_weight"] = 0.6
            weights["rule_weight"] = 0.2
        
        # 销售岗位：更看重规则匹配（硬门槛）
        elif "销售" in job_type or "市场" in job_type:
            weights["vector_weight"] = 0.2
            weights["llm_weight"] = 0.3
            weights["rule_weight"] = 0.5
        
        return weights
    
    def _calculate_rule_score(self, rule_match_result: Dict) -> float:
        """计算规则匹配分数"""
        if not rule_match_result:
            return 0.0
        
        total_rules = rule_match_result.get("total_rules", 0)
        passed_rules = rule_match_result.get("passed_rules", 0)
        
        if total_rules == 0:
            return 1.0  # 没有规则，默认通过
        
        return passed_rules / total_rules
    
    def _generate_match_label(self, score: float) -> str:
        """生成匹配标签"""
        if score >= 8.5:
            return "强烈推荐"
        elif score >= 7.0:
            return "推荐"
        elif score >= 6.0:
            return "谨慎推荐"
        else:
            return "不推荐"
```

### 4.2 向量相似度计算优化

**优化方案**：

```python
class VectorSimilarityCalculator:
    """向量相似度计算器"""
    
    def __init__(self, embedding_model: str = "deepseek"):
        self.embedding_model = embedding_model
        self.vector_dim = 768  # 根据模型调整
    
    async def generate_resume_vector(
        self,
        resume_data: Dict[str, Any]
    ) -> List[float]:
        """
        生成简历向量
        
        提取关键信息：
        - 工作经历（公司、职位、职责）
        - 技能（技术栈、工具）
        - 教育背景（学校、专业、学历）
        - 项目经验（项目名称、职责、成果）
        """
        # 构建向量化文本
        vector_text = self._build_resume_vector_text(resume_data)
        
        # 生成向量（调用Embedding API）
        vector = await self._generate_embedding(vector_text)
        
        return vector
    
    async def generate_job_vector(
        self,
        job: JobPosition,
        job_parsed_data: Optional[Dict[str, Any]] = None
    ) -> List[float]:
        """
        生成岗位向量
        
        提取关键信息：
        - 岗位要求（技能、经验、学历）
        - 岗位描述（职责、工作内容）
        - 部门信息（部门职责、部门特点）
        """
        # 构建向量化文本
        vector_text = self._build_job_vector_text(job, job_parsed_data)
        
        # 生成向量
        vector = await self._generate_embedding(vector_text)
        
        return vector
    
    def calculate_similarity(
        self,
        vector1: List[float],
        vector2: List[float],
        method: str = "cosine"
    ) -> float:
        """
        计算向量相似度
        
        Args:
            method: 相似度计算方法（cosine/euclidean/dot_product）
        """
        import numpy as np
        
        v1 = np.array(vector1)
        v2 = np.array(vector2)
        
        if method == "cosine":
            # 余弦相似度
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return max(0.0, similarity)  # 确保在0-1范围
        
        elif method == "euclidean":
            # 欧氏距离（转换为相似度）
            distance = np.linalg.norm(v1 - v2)
            similarity = 1 / (1 + distance)  # 距离越小，相似度越高
            return similarity
        
        elif method == "dot_product":
            # 点积相似度（需要向量已归一化）
            return np.dot(v1, v2)
        
        else:
            raise ValueError(f"Unknown similarity method: {method}")
    
    def _build_resume_vector_text(self, resume_data: Dict[str, Any]) -> str:
        """构建简历向量化文本"""
        parts = []
        
        # 工作经历
        work_experiences = resume_data.get("work_experiences", [])
        for exp in work_experiences:
            parts.append(f"工作: {exp.get('company', '')} {exp.get('position', '')}")
            if exp.get("responsibilities"):
                parts.append(f"职责: {', '.join(exp['responsibilities'])}")
        
        # 技能
        skills = resume_data.get("skills", {})
        if skills.get("technical"):
            parts.append(f"技能: {', '.join(skills['technical'])}")
        
        # 教育背景
        education = resume_data.get("education", [])
        for edu in education:
            parts.append(f"教育: {edu.get('school', '')} {edu.get('major', '')} {edu.get('degree', '')}")
        
        # 项目经验
        projects = resume_data.get("projects", [])
        for proj in projects:
            parts.append(f"项目: {proj.get('name', '')} {proj.get('description', '')}")
        
        return " | ".join(parts)
    
    def _build_job_vector_text(
        self,
        job: JobPosition,
        job_parsed_data: Optional[Dict[str, Any]] = None
    ) -> str:
        """构建岗位向量化文本"""
        parts = []
        
        # 岗位要求
        if job_parsed_data:
            requirements = job_parsed_data.get("requirements", {})
            if requirements.get("skills"):
                parts.append(f"技能要求: {', '.join(requirements['skills'])}")
            if requirements.get("experience", {}).get("fields"):
                parts.append(f"相关领域: {', '.join(requirements['experience']['fields'])}")
        
        # 岗位描述
        if job.description:
            parts.append(f"岗位描述: {job.description}")
        
        # 部门信息（如果有关联）
        if job.department_obj:
            parts.append(f"部门: {job.department_obj.name}")
            if job.department_obj.description:
                parts.append(f"部门职责: {job.department_obj.description}")
        
        return " | ".join(parts)
    
    async def _generate_embedding(self, text: str) -> List[float]:
        """
        生成文本向量
        
        TODO: 调用实际的Embedding API
        - DeepSeek Embedding API
        - 千问 Embedding API
        - 或使用本地模型（sentence-transformers）
        """
        # 占位符：实际实现需要调用Embedding API
        # 这里返回零向量作为示例
        return [0.0] * self.vector_dim
```

### 4.3 批量处理优化

**优化方案**：

```python
class BatchProcessor:
    """批量处理器（并发优化）"""
    
    def __init__(self, max_concurrent: int = 5):
        self.max_concurrent = max_concurrent
    
    async def batch_parse_resumes(
        self,
        resume_ids: List[int],
        semaphore: Optional[asyncio.Semaphore] = None
    ) -> List[Dict[str, Any]]:
        """
        批量解析简历（并发处理）
        """
        if semaphore is None:
            semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def parse_single(resume_id):
            async with semaphore:
                try:
                    return await self._parse_resume(resume_id)
                except Exception as e:
                    logger.error(f"解析简历失败: resume_id={resume_id}, error={e}")
                    return {"resume_id": resume_id, "error": str(e)}
        
        tasks = [parse_single(rid) for rid in resume_ids]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results
    
    async def batch_match_resumes(
        self,
        resume_ids: List[int],
        job_id: int,
        semaphore: Optional[asyncio.Semaphore] = None
    ) -> List[Dict[str, Any]]:
        """
        批量匹配简历（并发处理）
        """
        if semaphore is None:
            semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def match_single(resume_id):
            async with semaphore:
                try:
                    return await self._match_resume_to_job(resume_id, job_id)
                except Exception as e:
                    logger.error(f"匹配简历失败: resume_id={resume_id}, job_id={job_id}, error={e}")
                    return {"resume_id": resume_id, "error": str(e)}
        
        tasks = [match_single(rid) for rid in resume_ids]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results
```

### 4.4 简历去重算法优化

**优化方案**：

```python
class ResumeDeduplication:
    """简历去重算法"""
    
    def __init__(self, similarity_threshold: float = 0.95):
        self.similarity_threshold = similarity_threshold
    
    def is_duplicate(
        self,
        resume1: Dict[str, Any],
        resume2: Dict[str, Any]
    ) -> Tuple[bool, float]:
        """
        判断两个简历是否重复
        
        Returns:
            (是否重复, 相似度)
        """
        # 方法1：关键信息匹配（快速检查）
        key_info_similarity = self._compare_key_info(resume1, resume2)
        if key_info_similarity >= 0.98:
            return True, key_info_similarity
        
        # 方法2：文本相似度（详细检查）
        text_similarity = self._calculate_text_similarity(resume1, resume2)
        
        # 方法3：特征向量相似度（如果有关键特征）
        # feature_similarity = self._calculate_feature_similarity(resume1, resume2)
        
        # 综合相似度
        final_similarity = max(key_info_similarity, text_similarity)
        
        is_dup = final_similarity >= self.similarity_threshold
        
        return is_dup, final_similarity
    
    def _compare_key_info(
        self,
        resume1: Dict[str, Any],
        resume2: Dict[str, Any]
    ) -> float:
        """比较关键信息（姓名、电话、邮箱）"""
        basic1 = resume1.get("basic_info", {})
        basic2 = resume2.get("basic_info", {})
        
        matches = 0
        total = 0
        
        # 姓名匹配
        if basic1.get("name") and basic2.get("name"):
            total += 1
            if basic1["name"] == basic2["name"]:
                matches += 1
        
        # 电话匹配
        if basic1.get("phone") and basic2.get("phone"):
            total += 1
            if basic1["phone"] == basic2["phone"]:
                matches += 1
        
        # 邮箱匹配
        if basic1.get("email") and basic2.get("email"):
            total += 1
            if basic1["email"] == basic2["email"]:
                matches += 1
        
        if total == 0:
            return 0.0
        
        return matches / total
    
    def _calculate_text_similarity(
        self,
        resume1: Dict[str, Any],
        resume2: Dict[str, Any]
    ) -> float:
        """计算文本相似度（Jaccard相似度）"""
        # 提取文本特征
        text1 = self._extract_text_features(resume1)
        text2 = self._extract_text_features(resume2)
        
        # Jaccard相似度
        set1 = set(text1.split())
        set2 = set(text2.split())
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        if union == 0:
            return 0.0
        
        return intersection / union
    
    def _extract_text_features(self, resume_data: Dict[str, Any]) -> str:
        """提取文本特征"""
        parts = []
        
        # 工作经历
        for exp in resume_data.get("work_experiences", []):
            parts.append(exp.get("company", ""))
            parts.append(exp.get("position", ""))
        
        # 技能
        for skill in resume_data.get("skills", {}).get("technical", []):
            parts.append(skill)
        
        # 教育背景
        for edu in resume_data.get("education", []):
            parts.append(edu.get("school", ""))
            parts.append(edu.get("major", ""))
        
        return " ".join(parts)
```

## 五、算法实施优先级

### 5.1 第一阶段（核心算法）

1. **匹配度融合算法优化** - 影响匹配精度
2. **向量相似度计算优化** - 影响匹配精度
3. **批量处理优化** - 影响处理效率

### 5.2 第二阶段（体验优化）

4. **简历去重算法优化** - 提升数据质量
5. **排序算法优化** - 提升用户体验
6. **规则匹配算法优化** - 提升筛选精度

### 5.3 第三阶段（高级优化）

7. **文本特征提取优化**
8. **异常检测算法**
9. **机器学习融合模型**（使用历史数据训练）

## 六、总结

### 6.1 核心算法

1. **匹配度融合算法** - 最重要，直接影响匹配精度
2. **向量相似度计算** - 核心算法，影响匹配基础
3. **批量处理优化** - 影响系统性能和用户体验

### 6.2 优化方向

- **精度优化**：匹配度融合、向量相似度
- **效率优化**：批量处理、并发处理
- **质量优化**：去重算法、异常检测

### 6.3 实施建议

建议优先实施：
1. 匹配度融合算法优化（动态权重、多维度融合）
2. 向量相似度计算优化（统一向量生成、优化相似度计算）
3. 批量处理优化（并发处理、任务调度）

这样可以快速提升系统的匹配精度和处理效率。

